{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f911c6",
   "metadata": {},
   "source": [
    "# Python Crash Course - Chapter 16: Downloading Data\n",
    "\n",
    "This notebook contains exercises from Chapter 16 of Python Crash Course by Eric Matthes. This chapter focuses on downloading data from online sources, working with APIs, and processing web-based datasets for visualization and analysis.\n",
    "\n",
    "## Learning Objectives:\n",
    "- Download data from web APIs using the requests library\n",
    "- Work with CSV files and data parsing\n",
    "- Process JSON data from web services\n",
    "- Handle API responses and error conditions\n",
    "- Create visualizations from real-world datasets\n",
    "- Work with datetime data and time series\n",
    "- Understand API rate limiting and best practices\n",
    "- Process large datasets efficiently\n",
    "\n",
    "---\n",
    "\n",
    "## Setup: Required Imports\n",
    "\n",
    "First, let's import the libraries we'll need for this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports for Chapter 16 exercises\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.error import URLError\n",
    "\n",
    "# Test imports and show versions\n",
    "print(f\"Requests version: {requests.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(\"All imports successful!\")\n",
    "print(\"Ready to download and visualize web data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c11c2",
   "metadata": {},
   "source": [
    "## 16-1 San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f750ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16-1: San Francisco\n",
    "# Are temperatures in San Francisco more like temperatures in Sitka or temperatures in Death Valley?\n",
    "# Download some data for San Francisco, and generate a high-low temperature plot for San Francisco\n",
    "# to make a comparison.\n",
    "\n",
    "# Note: You'll need to find a weather data source or use historical weather data\n",
    "# Example sources: OpenWeatherMap API, NOAA data, or local CSV files\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0fbff",
   "metadata": {},
   "source": [
    "## 16-2 Sitka-Death Valley Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73deebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16-2: Sitka-Death Valley Comparison\n",
    "# The temperature scales on the Sitka and Death Valley graphs reflect the different ranges\n",
    "# of the data. To accurately compare the temperature range in Sitka to that of Death Valley,\n",
    "# you need identical scales on the y-axis. Change the settings for the y-axis on one or both\n",
    "# of the charts in Figures 16-5 and 16-6, and make a direct comparison between temperature\n",
    "# ranges in Sitka and Death Valley (or any two places you want to compare).\n",
    "\n",
    "# Sample data structure for weather data\n",
    "sitka_data = {\n",
    "    'dates': [],\n",
    "    'highs': [],\n",
    "    'lows': []\n",
    "}\n",
    "\n",
    "death_valley_data = {\n",
    "    'dates': [],\n",
    "    'highs': [],\n",
    "    'lows': []\n",
    "}\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35416d36",
   "metadata": {},
   "source": [
    "## 16-3 Rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16-3: Rainfall\n",
    "# Choose a location you're curious about, and make a visualization that plots its rainfall.\n",
    "# Start by focusing on one month's data, and then once your code is working,\n",
    "# see if you can pull in a full year's worth of data.\n",
    "\n",
    "def parse_rainfall_data(filename):\n",
    "    \"\"\"Parse rainfall data from CSV file.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def plot_rainfall(dates, rainfall_amounts, location):\n",
    "    \"\"\"Create a rainfall visualization.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abad74",
   "metadata": {},
   "source": [
    "## 16-4 Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16-4: Explore\n",
    "# Generate a few more visualizations that examine any other weather aspect you're curious about\n",
    "# for any locations you're curious about.\n",
    "\n",
    "# Ideas for exploration:\n",
    "# - Wind speed patterns\n",
    "# - Humidity levels\n",
    "# - Pressure changes\n",
    "# - Seasonal variations\n",
    "# - Multiple cities comparison\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3aa452",
   "metadata": {},
   "source": [
    "## 16-5 Testing python-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16-5: Testing python-requests\n",
    "# Visit the home page for the python-requests project (at https://requests.readthedocs.io/)\n",
    "# and look at the status of the project. In particular, look at the Issues and Pull Requests\n",
    "# to get a sense of the project's activity.\n",
    "\n",
    "# Test the requests library with a simple API call\n",
    "def test_requests_library():\n",
    "    \"\"\"Test the requests library with a simple API call.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f90af",
   "metadata": {},
   "source": [
    "## 16-6 Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911abe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16-6: Refactoring\n",
    "# The loop that pulls data from all_python_repos.json is getting pretty long.\n",
    "# Create a function called get_repo_dict() that takes one repository dictionary and\n",
    "# returns the values you're plotting. Call this function once for each repository dictionary.\n",
    "\n",
    "def get_repo_dict(repo):\n",
    "    \"\"\"Extract relevant information from a repository dictionary.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def fetch_python_repos():\n",
    "    \"\"\"Fetch Python repositories from GitHub API.\"\"\"\n",
    "    # GitHub API endpoint for Python repositories\n",
    "    url = 'https://api.github.com/search/repositories'\n",
    "    \n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def plot_repo_data(repo_data):\n",
    "    \"\"\"Create visualization of repository data.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0b344",
   "metadata": {},
   "source": [
    "## 16-7 Automated Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f74d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16-7: Automated Testing\n",
    "# When testing whether the key 'stargazers_count' is in the repository dictionary,\n",
    "# write a test that will pass whether the key is 'stargazers_count' or 'watchers_count'.\n",
    "# This will help make your code more robust, because GitHub sometimes uses one\n",
    "# term and sometimes the other.\n",
    "\n",
    "def safe_get_stars(repo):\n",
    "    \"\"\"Safely get star count from repository data, handling different key names.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def test_repo_data_access():\n",
    "    \"\"\"Test different ways of accessing repository data.\"\"\"\n",
    "    # Create test repository data with different key structures\n",
    "    test_repo_1 = {\n",
    "        'name': 'test-repo-1',\n",
    "        'stargazers_count': 1500,\n",
    "        'html_url': 'https://github.com/test/repo1'\n",
    "    }\n",
    "    \n",
    "    test_repo_2 = {\n",
    "        'name': 'test-repo-2',\n",
    "        'watchers_count': 2000,\n",
    "        'html_url': 'https://github.com/test/repo2'\n",
    "    }\n",
    "    \n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd1cf31",
   "metadata": {},
   "source": [
    "## 16-8 Recent Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16-8: Recent Repositories\n",
    "# Modify the API call in python_repos.py so it generates a chart showing the most recently\n",
    "# created Python projects on GitHub.\n",
    "\n",
    "def fetch_recent_python_repos():\n",
    "    \"\"\"Fetch recently created Python repositories from GitHub API.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def parse_creation_dates(repos):\n",
    "    \"\"\"Parse and format repository creation dates.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def plot_recent_repos(repos):\n",
    "    \"\"\"Create visualization of recently created repositories.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a2fc8",
   "metadata": {},
   "source": [
    "## 16-9 Testing python-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16-9: Testing python-requests\n",
    "# Look at the information returned by the API call in python_repos.py.\n",
    "# Make a chart showing Python projects that have the most forks.\n",
    "\n",
    "def fetch_most_forked_repos():\n",
    "    \"\"\"Fetch Python repositories sorted by fork count.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def plot_fork_data(repos):\n",
    "    \"\"\"Create visualization of most forked repositories.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea20023",
   "metadata": {},
   "source": [
    "## Working with API Keys and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820db6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice working with APIs that require authentication\n",
    "# Learn best practices for handling API keys securely\n",
    "\n",
    "import os\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "class APIClient:\n",
    "    \"\"\"A generic API client with authentication support.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url, api_key=None):\n",
    "        \"\"\"Initialize API client.\"\"\"\n",
    "        # Here I will write the code and corresponding comments to complete the training tasks\n",
    "        pass\n",
    "    \n",
    "    def make_request(self, endpoint, params=None):\n",
    "        \"\"\"Make authenticated API request.\"\"\"\n",
    "        # Here I will write the code and corresponding comments to complete the training tasks\n",
    "        pass\n",
    "    \n",
    "    def handle_rate_limit(self, response):\n",
    "        \"\"\"Handle API rate limiting gracefully.\"\"\"\n",
    "        # Here I will write the code and corresponding comments to complete the training tasks\n",
    "        pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546feb13",
   "metadata": {},
   "source": [
    "## Error Handling and Robust API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice robust error handling for web requests\n",
    "# Learn to handle network errors, API errors, and data parsing issues\n",
    "\n",
    "def robust_api_call(url, params=None, max_retries=3):\n",
    "    \"\"\"Make a robust API call with error handling and retries.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def validate_api_response(response_data, required_fields):\n",
    "    \"\"\"Validate that API response contains required fields.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def safe_data_extraction(data, key_path):\n",
    "    \"\"\"Safely extract nested data from API responses.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a515a",
   "metadata": {},
   "source": [
    "## Working with Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a89d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice techniques for handling large datasets from APIs\n",
    "# Learn pagination, streaming, and memory-efficient processing\n",
    "\n",
    "def paginated_api_call(base_url, params=None, page_size=100):\n",
    "    \"\"\"Handle paginated API responses to get all data.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def stream_large_dataset(url, chunk_size=1024):\n",
    "    \"\"\"Stream large files without loading everything into memory.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "def process_data_in_chunks(data, chunk_size=1000):\n",
    "    \"\"\"Process large datasets in manageable chunks.\"\"\"\n",
    "    # Here I will write the code and corresponding comments to complete the training tasks\n",
    "    pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471466a",
   "metadata": {},
   "source": [
    "## Data Caching and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ea645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement caching strategies to improve performance and reduce API calls\n",
    "# Learn when and how to cache API responses\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "class DataCache:\n",
    "    \"\"\"Simple file-based cache for API responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir='cache', cache_duration=3600):\n",
    "        \"\"\"Initialize cache with directory and duration settings.\"\"\"\n",
    "        # Here I will write the code and corresponding comments to complete the training tasks\n",
    "        pass\n",
    "    \n",
    "    def get(self, key):\n",
    "        \"\"\"Get cached data if it exists and is not expired.\"\"\"\n",
    "        # Here I will write the code and corresponding comments to complete the training tasks\n",
    "        pass\n",
    "    \n",
    "    def set(self, key, data):\n",
    "        \"\"\"Cache data with timestamp.\"\"\"\n",
    "        # Here I will write the code and corresponding comments to complete the training tasks\n",
    "        pass\n",
    "    \n",
    "    def is_expired(self, timestamp):\n",
    "        \"\"\"Check if cached data has expired.\"\"\"\n",
    "        # Here I will write the code and corresponding comments to complete the training tasks\n",
    "        pass\n",
    "\n",
    "# Here I will write the code and corresponding comments to complete the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5f870",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've completed all the exercises for Chapter 16 on Downloading Data. You should now be comfortable with:\n",
    "\n",
    "**Key Concepts Practiced:**\n",
    "- **Web APIs**: Making HTTP requests to retrieve data from online services\n",
    "- **JSON Processing**: Parsing and extracting data from JSON API responses\n",
    "- **CSV Data Handling**: Reading and processing comma-separated value files\n",
    "- **Data Visualization**: Creating charts from real-world datasets\n",
    "- **Error Handling**: Robust programming practices for network operations\n",
    "- **Authentication**: Working with API keys and secure access methods\n",
    "\n",
    "**Technical Skills Developed:**\n",
    "- **HTTP Requests**: Using the requests library for web data access\n",
    "- **API Integration**: Understanding REST APIs and response handling\n",
    "- **Data Parsing**: Converting raw data into usable Python structures\n",
    "- **Date/Time Processing**: Working with timestamps and time series data\n",
    "- **Performance Optimization**: Caching, pagination, and efficient data processing\n",
    "- **Security Best Practices**: Safe handling of API credentials and rate limiting\n",
    "\n",
    "**Real-World Applications:**\n",
    "- **Weather Analysis**: Processing meteorological data for insights\n",
    "- **Social Media Analytics**: Analyzing trends and engagement metrics\n",
    "- **Financial Data**: Stock prices, market trends, and economic indicators\n",
    "- **Scientific Research**: Accessing research databases and datasets\n",
    "- **Business Intelligence**: Integrating external data sources for analysis\n",
    "- **IoT and Sensors**: Collecting and processing sensor data streams\n",
    "\n",
    "**Programming Best Practices:**\n",
    "- **Code Refactoring**: Breaking complex operations into reusable functions\n",
    "- **Error Recovery**: Handling network failures and malformed data gracefully\n",
    "- **Testing Strategies**: Validating data integrity and API responses\n",
    "- **Memory Management**: Efficient processing of large datasets\n",
    "- **Documentation**: Clear code comments and function documentation\n",
    "\n",
    "**Professional Development:**\n",
    "- **API Documentation**: Reading and understanding API specifications\n",
    "- **Rate Limiting**: Respecting service limitations and usage policies\n",
    "- **Data Ethics**: Understanding terms of service and data usage rights\n",
    "- **Monitoring**: Tracking API usage and performance metrics\n",
    "- **Scalability**: Designing systems that can handle growing data needs\n",
    "\n",
    "**Next Steps:**\n",
    "- Explore additional APIs in domains that interest you\n",
    "- Practice with different data formats (XML, GraphQL, Protocol Buffers)\n",
    "- Learn about more advanced authentication methods (OAuth, JWT)\n",
    "- Move on to Chapter 17: Working with APIs\n",
    "- Consider building a complete data pipeline project\n",
    "\n",
    "**Advanced Topics to Explore:**\n",
    "- **Async Programming**: Using async/await for concurrent API calls\n",
    "- **Database Integration**: Storing API data in databases for analysis\n",
    "- **Data Streaming**: Real-time data processing and visualization\n",
    "- **Machine Learning**: Using downloaded data for predictive modeling\n",
    "- **Web Scraping**: Extracting data from websites without APIs\n",
    "\n",
    "---\n",
    "\n",
    "*Note: Working with external data sources is a fundamental skill in modern programming. The ability to integrate, process, and visualize real-world data opens up countless possibilities for meaningful applications. These skills are essential for data science, web development, scientific computing, and business applications. Keep practicing with different APIs and data sources to build your expertise!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
